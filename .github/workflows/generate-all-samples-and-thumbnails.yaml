name: Generate All Samples and Cache Thumbnails

on:
  push:
    branches:
      - main
    paths:
      - ".metadata/samples.json"
      - ".metadata/extensions-samples.json"
  workflow_dispatch:
  schedule:
    - cron: "0 6 * * *" # Daily 06:00 UTC

jobs:
  merge_all_samples:
    runs-on: ubuntu-latest

    permissions:
      contents: write

    steps:
      - name: Checkout main (source JSON)
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          ref: main
          fetch-depth: 0

      - name: Checkout Docs branch (published site output)
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          ref: Docs
          path: Docs
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Set up Node
        uses: actions/setup-node@v4
        with:
          node-version: "20"

-
      - name: Merge + slim samples files
        id: merge
        run: |
          python3 << 'EOF'
          import json
          import sys
          from pathlib import Path

          def flatten_samples(data):
              if isinstance(data, list):
                  out = []
                  for item in data:
                      if isinstance(item, dict):
                          out.append(item)
                      elif isinstance(item, list):
                          out.extend(flatten_samples(item))
                  return out
              if isinstance(data, dict):
                  return [data]
              return []

          def pick_primary_thumbnail(sample: dict):
              thumbs = sample.get("thumbnails") or []
              if not isinstance(thumbs, list):
                  return None

              images = [t for t in thumbs if isinstance(t, dict) and t.get("type") == "image" and t.get("url")]
              if not images:
                  return None

              def order_key(t):
                  o = t.get("order")
                  return o if isinstance(o, int) else 999999

              images.sort(key=order_key)
              return images[0]

          def slim_sample(sample: dict):
              primary = pick_primary_thumbnail(sample)

              out = {
                  "name": sample.get("name"),
                  "source": sample.get("source"),
                  "title": sample.get("title"),
                  "shortDescription": sample.get("shortDescription"),
                  "url": sample.get("url"),
                  "downloadUrl": sample.get("downloadUrl"),
                  "updateDateTime": sample.get("updateDateTime"),
                  "metadata": sample.get("metadata") or [],
                  "tags": sample.get("tags") or [],
                  "categories": sample.get("categories") or [],
                  "authors": sample.get("authors") or [],
                  "thumbnails":  [primary] if primary else [],
              }
              return out

          all_samples = []
          errors = []
          file_stats = {}

          base_path = Path.cwd().resolve()
          metadata_dir = base_path / ".metadata"
          metadata_dir.mkdir(parents=True, exist_ok=True)

          files_to_merge = [
              ("samples.json", "WebParts"),
              ("extensions-samples.json", "Extensions")
          ]

          print("üîÑ Merging + slimming sample files...\n")

          for filename, source_type in files_to_merge:
              file_path = metadata_dir / filename
              if not file_path.exists():
                  print(f"‚ÑπÔ∏è {filename}: File not found (skipping)")
                  file_stats[filename] = {"type": source_type, "count": 0, "status": "not_found"}
                  continue

              try:
                  content = file_path.read_text(encoding="utf-8").strip()
                  if not content:
                      errors.append({"file": filename, "error": "Empty file"})
                      print(f"‚úó {filename}: Empty file")
                      file_stats[filename] = {"type": source_type, "count": 0, "status": "empty"}
                      continue

                  data = json.loads(content)
                  samples = flatten_samples(data)
                  if not samples:
                      errors.append({"file": filename, "error": "No valid samples found after flattening"})
                      print(f"‚úó {filename}: No valid samples found")
                      file_stats[filename] = {"type": source_type, "count": 0, "status": "no_samples"}
                      continue

                  slimmed = [slim_sample(s) for s in samples if isinstance(s, dict)]
                  all_samples.extend(slimmed)
                  print(f"‚úì {filename}: Added {len(slimmed)} {source_type} samples")
                  file_stats[filename] = {"type": source_type, "count": len(slimmed), "status": "success"}

              except json.JSONDecodeError as e:
                  errors.append({"file": filename, "error": f"JSON decode error: {str(e)}"})
                  print(f"‚úó {filename}: JSON error - {str(e)}")
                  file_stats[filename] = {"type": source_type, "count": 0, "status": "json_error"}
              except Exception as e:
                  errors.append({"file": filename, "error": f"Unexpected error: {str(e)}"})
                  print(f"‚úó {filename}: Error - {str(e)}")
                  file_stats[filename] = {"type":  source_type, "count": 0, "status": "error"}

          out_path = metadata_dir / "all-samples.json"
          try:
              out_path.write_text(json.dumps(all_samples, indent=2, ensure_ascii=False), encoding="utf-8")
              print(f"\n‚úì Successfully wrote {out_path}")
          except Exception as e:
              print(f"\n‚úó Error writing {out_path}: {str(e)}")
              sys.exit(1)

          merge_errors_file = metadata_dir / "merge-errors.json"
          if errors:
              payload = {
                  "timestamp": "${{ github.event.head_commit.timestamp }}",
                  "commit": "${{ github.sha }}",
                  "errors": errors
              }
              merge_errors_file.write_text(json.dumps(payload, indent=2, ensure_ascii=False), encoding="utf-8")
              print(f"‚ö†Ô∏è Errors written to: {merge_errors_file}")
              has_errors = "true"
          else:
              if merge_errors_file.exists():
                  merge_errors_file.unlink()
                  print("‚úì No errors.  Removed existing merge-errors.json")
              has_errors = "false"

          import os
          with open(os.environ["GITHUB_OUTPUT"], "a") as f:
              f.write(f"has_errors={has_errors}\n")
              f.write(f"total_samples={len(all_samples)}\n")
              f.write(f"file_stats={json.dumps(file_stats)}\n")
          EOF

      - name: Install image optimizer deps
        run: |
          npm init -y >/dev/null 2>&1
          npm install sharp@latest

      - name: Optimize thumbnails + rewrite all-samples.json thumbnails (publish to Docs branch)
        id: thumbs
        run: |
          node << 'EOF'
          const fs = require("fs");
          const path = require("path");
          const sharp = require("sharp");

          const repo = process.env.GITHUB_REPOSITORY;        // owner/name
          const repoName = repo.split("/")[1];               // name
          const pagesBase = `/${repoName}/`;                 // GitHub Pages project base
          const thumbUrlBase = `${pagesBase}images/thumbnails/`;

          const allSamplesPath = path.resolve(".metadata/all-samples.json");
          const samples = JSON.parse(fs.readFileSync(allSamplesPath, "utf8"));

          // Write directly into Docs branch published site output
          const thumbsDir = path.resolve("Docs/docs/images/thumbnails");
          fs.mkdirSync(thumbsDir, { recursive: true });

          const manifestPath = path.join(thumbsDir, "manifest.json");
          const oldManifest = fs.existsSync(manifestPath)
            ? JSON.parse(fs.readFileSync(manifestPath, "utf8"))
            : {};

          const newManifest = {};
          const expectedFiles = new Set(["manifest.json"]);
          const failedThumbs = [];

          function safeFileName(name) {
            return String(name).replace(/[^a-zA-Z0-9._-]/g, "_");
          }

          function pickThumb(sample) {
            const thumbs = Array.isArray(sample.thumbnails) ? sample.thumbnails : [];
            const img = thumbs
              .filter(t => t && t.type === "image" && typeof t.url === "string" && t.url.length > 0)
              .sort((a, b) => (a.order ?? 999999) - (b.order ?? 999999))[0];
            return img || null;
          }

          async function fetchBuffer(url) {
            const res = await fetch(url, { redirect: "follow" });
            if (!res.ok) throw new Error(`HTTP ${res.status}`);
            const ab = await res.arrayBuffer();
            return Buffer.from(ab);
          }

          async function makeThumbWebp(inputBuffer, outPath) {
            const maxWidth = 420;
            let img;
            try {
              img = sharp(inputBuffer, { animated: true, pages: 1 });
              await img.metadata();
            } catch {
              img = sharp(inputBuffer);
            }

            await img
              .resize({ width: maxWidth, withoutEnlargement: true })
              .webp({ quality: 72, effort: 4 })
              .toFile(outPath);
          }

          async function run() {
            let processed = 0, skipped = 0, failed = 0;

            for (const s of samples) {
              const name = s?.name;
              if (!name) continue;

              const thumb = pickThumb(s);
              const origUrl = thumb?.url;

              const fileBase = safeFileName(name);
              const outFile = `${fileBase}.webp`;
              const outPath = path.join(thumbsDir, outFile);

              expectedFiles.add(outFile);

              if (!origUrl) {
                s.thumbnails = [];
                continue;
              }

              newManifest[name] = { sourceUrl: origUrl, file: outFile };

              const prev = oldManifest[name];
              const unchanged = prev && prev.sourceUrl === origUrl && fs.existsSync(outPath);

              if (unchanged) {
                skipped++;
              } else {
                try {
                  const buf = await fetchBuffer(origUrl);
                  await makeThumbWebp(buf, outPath);
                  processed++;
                } catch (e) {
                  failed++;
                  failedThumbs.push({ name, url: origUrl, error: e.message });
                  // keep original URL if processing fails
                  const alt = thumb?.alt || s.title || "Sample thumbnail";
                  s.thumbnails = [{ type: "image", order: 100, url: origUrl, alt }];
                  continue;
                }
              }

              const alt = thumb?.alt || s.title || "Sample thumbnail";
              s.thumbnails = [{
                type: "image",
                order: 100,
                url: thumbUrlBase + encodeURIComponent(outFile),
                alt
              }];
            }

            // delete orphans in Docs/docs/images/thumbnails
            let deleted = 0;
            for (const file of fs.readdirSync(thumbsDir)) {
              if (!expectedFiles.has(file)) {
                fs.rmSync(path.join(thumbsDir, file), { force: true });
                deleted++;
              }
            }

            fs.writeFileSync(manifestPath, JSON.stringify(newManifest, null, 2), "utf8");
            fs.writeFileSync(allSamplesPath, JSON.stringify(samples, null, 2), "utf8");

            fs.appendFileSync(process.env.GITHUB_OUTPUT, `thumbs_processed=${processed}\n`);
            fs.appendFileSync(process.env.GITHUB_OUTPUT, `thumbs_skipped=${skipped}\n`);
            fs.appendFileSync(process.env.GITHUB_OUTPUT, `thumbs_failed=${failed}\n`);
            fs.appendFileSync(process.env.GITHUB_OUTPUT, `thumbs_deleted=${deleted}\n`);
          }

          run().catch(err => { console.error(err); process.exit(1); });
          EOF

      - name: Normalize author avatars + rewrite all-samples.json authors.pictureUrl (publish to Docs branch)
        id: avatars
        run: |
          node << 'EOF'
          const fs = require("fs");
          const path = require("path");
          const crypto = require("crypto");
          const sharp = require("sharp");

          const repo = process.env.GITHUB_REPOSITORY;
          const repoName = repo.split("/")[1];
          const pagesBase = `/${repoName}/`;
          const avatarUrlBase = `${pagesBase}images/avatars/`;

          const allSamplesPath = path.resolve(".metadata/all-samples.json");
          const samples = JSON.parse(fs.readFileSync(allSamplesPath, "utf8"));

          // Write directly into Docs branch published site output
          const avatarsDir = path.resolve("Docs/docs/images/avatars");
          fs.mkdirSync(avatarsDir, { recursive: true });

          const manifestPath = path.join(avatarsDir, "manifest.json");
          const oldManifest = fs.existsSync(manifestPath)
            ? JSON.parse(fs.readFileSync(manifestPath, "utf8"))
            : {}; // keyed by sourceUrl

          function safeSlug(s) {
            return String(s || "author")
              .trim()
              .toLowerCase()
              .replace(/[^a-z0-9]+/g, "-")
              .replace(/^-+|-+$/g, "")
              .slice(0, 40) || "author";
          }

          function sha256Hex(buf) {
            return crypto.createHash("sha256").update(buf).digest("hex");
          }

          async function fetchBuffer(url) {
            const res = await fetch(url, { redirect: "follow" });
            if (!res.ok) throw new Error(`HTTP ${res.status}`);
            const ab = await res.arrayBuffer();
            return {
              buf: Buffer.from(ab),
              contentType: res.headers.get("content-type") || ""
            };
          }

          async function normalizeToWebp(inputBuf, contentType, sizePx) {
            const isSvg = /image\/svg\+xml/i.test(contentType);
            const img = isSvg
              ? sharp(inputBuf, { density: 256 })
              : sharp(inputBuf, { animated: true, pages: 1 });

            return await img
              .resize(sizePx, sizePx, { fit: "cover", position: "centre" })
              .webp({ quality: 80, effort: 4 })
              .toBuffer();
          }

          async function run() {
            const sizePx = 28;

            // unique avatar URLs
            const byUrl = new Map();
            for (const s of samples) {
              const authors = Array.isArray(s.authors) ? s.authors : [];
              for (const a of authors) {
                const url = a?.pictureUrl;
                if (typeof url !== "string" || !url.startsWith("http")) continue;
                const slugBase = a.gitHubAccount || a.name || "author";
                if (!byUrl.has(url)) byUrl.set(url, { slugBase });
              }
            }

            const expectedFiles = new Set(["manifest.json"]);
            const newManifest = {};
            let processed = 0, skipped = 0, failed = 0, deleted = 0;

            for (const [sourceUrl, info] of byUrl.entries()) {
              try {
                const prev = oldManifest[sourceUrl];
                if (prev?.file) {
                  const prevPath = path.join(avatarsDir, prev.file);
                  if (fs.existsSync(prevPath)) {
                    newManifest[sourceUrl] = prev;
                    expectedFiles.add(prev.file);
                    skipped++;
                    continue;
                  }
                }

                const { buf, contentType } = await fetchBuffer(sourceUrl);
                const outBuf = await normalizeToWebp(buf, contentType, sizePx);

                const hash = sha256Hex(outBuf).slice(0, 12);
                const slug = safeSlug(info.slugBase);
                const outFile = `${slug}.${hash}.webp`;
                const outPath = path.join(avatarsDir, outFile);

                if (!fs.existsSync(outPath)) fs.writeFileSync(outPath, outBuf);

                newManifest[sourceUrl] = {
                  sourceUrl,
                  file: outFile,
                  sizePx,
                  sha256_12: hash
                };

                expectedFiles.add(outFile);
                processed++;
              } catch (e) {
                failed++;
                console.error(`‚ùå Avatar failed: ${sourceUrl}: ${e?.message || e}`);
              }
            }

            for (const file of fs.readdirSync(avatarsDir)) {
              if (!expectedFiles.has(file)) {
                fs.rmSync(path.join(avatarsDir, file), { force: true });
                deleted++;
              }
            }

            // rewrite authors[].pictureUrl
            for (const s of samples) {
              const authors = Array.isArray(s.authors) ? s.authors : [];
              for (const a of authors) {
                const sourceUrl = a?.pictureUrl;
                if (typeof sourceUrl !== "string" || !sourceUrl.startsWith("http")) continue;

                const entry = newManifest[sourceUrl] || oldManifest[sourceUrl];
                if (!entry?.file) continue;

                if (!a.avatarSourceUrl) a.avatarSourceUrl = sourceUrl;
                a.pictureUrl = avatarUrlBase + encodeURIComponent(entry.file);
              }
            }

            fs.writeFileSync(manifestPath, JSON.stringify(newManifest, null, 2), "utf8");
            fs.writeFileSync(allSamplesPath, JSON.stringify(samples, null, 2), "utf8");

            fs.appendFileSync(process.env.GITHUB_OUTPUT, `avatars_processed=${processed}\n`);
            fs.appendFileSync(process.env.GITHUB_OUTPUT, `avatars_skipped=${skipped}\n`);
            fs.appendFileSync(process.env.GITHUB_OUTPUT, `avatars_failed=${failed}\n`);
            fs.appendFileSync(process.env.GITHUB_OUTPUT, `avatars_deleted=${deleted}\n`);
          }

          run().catch(err => { console.error(err); process.exit(1); });
          EOF

      - name: Write published JSON files into Docs/docs/data
        id: publish_json
        run: |
          node << 'EOF'
          const fs = require("fs");
          const path = require("path");

          const allSamplesPath = path.resolve(".metadata/all-samples.json");
          const samples = JSON.parse(fs.readFileSync(allSamplesPath, "utf8"));

          const dataDir = path.resolve("Docs/docs/data");
          fs.mkdirSync(dataDir, { recursive: true });

          // all-samples.json
          fs.writeFileSync(
            path.join(dataDir, "all-samples.json"),
            JSON.stringify(samples, null, 2),
            "utf8"
          );

          // authors.json (deduped)
          const byKey = new Map();
          for (const s of samples) {
            const authors = Array.isArray(s.authors) ? s.authors : [];
            for (const a of authors) {
              if (!a) continue;
              const key =
                (a.gitHubAccount && `gh:${a.gitHubAccount.toLowerCase()}`) ||
                (a.avatarSourceUrl && `src:${a.avatarSourceUrl}`) ||
                (a.pictureUrl && `pic:${a.pictureUrl}`) ||
                (a.name && `name:${a.name.toLowerCase()}`);

              if (!key) continue;

              const existing = byKey.get(key) || {};
              byKey.set(key, {
                gitHubAccount: a.gitHubAccount || existing.gitHubAccount || "",
                name: a.name || existing.name || "",
                company: a.company || existing.company || "",
                pictureUrl: a.pictureUrl || existing.pictureUrl || "",
                avatarSourceUrl: a.avatarSourceUrl || existing.avatarSourceUrl || ""
              });
            }
          }

          const authors = Array.from(byKey.values())
            .sort((x, y) => (x.name || "").localeCompare(y.name || ""));

          fs.writeFileSync(
            path.join(dataDir, "authors.json"),
            JSON.stringify(authors, null, 2),
            "utf8"
          );

          // optional tiny meta file (handy for debugging)
          fs.writeFileSync(
            path.join(dataDir, "generated-meta.json"),
            JSON.stringify({ generatedAt: new Date().toISOString(), sampleCount: samples.length }, null, 2),
            "utf8"
          );
          EOF

      - name: Commit + push to Docs branch only
        run: |
          cd Docs
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"

          git add docs/images/avatars docs/images/thumbnails docs/data/all-samples.json docs/data/authors.json docs/data/generated-meta.json

          if git diff --cached --exit-code; then
            echo "‚ÑπÔ∏è No changes to publish to Docs branch"
            exit 0
          fi

          git commit -m "chore: publish samples data + images [skip ci]"
          git push
          echo "‚úÖ Changes committed and pushed"

      - name: Generate workflow summary
        if: always()
        run: |
          cat << 'EOF' >> $GITHUB_STEP_SUMMARY
          # üìä Merge All Samples Workflow Summary
          
          ## Workflow Details
          - **Triggered by**: ${{ github.event_name }}
          - **Run number**: ${{ github.run_number }}
          - **Commit**:  ${{ github.sha }}
          - **Branch**: ${{ github.ref_name }}
          
          ## üì¶ Sample Processing
          
          | Metric | Value |
          |--------|-------|
          | **Total Samples** | ${{ steps.merge.outputs.total_samples }} |
          | **Merge Errors** | ${{ steps.merge.outputs.has_errors == 'true' && '‚ö†Ô∏è Yes' || '‚úÖ No' }} |
          
          ## üßë‚Äçüé® Avatar Normalization

          | Metric | Count |
          |--------|-------|
          | **Processed** | ${{ steps.avatars.outputs.avatars_processed }} |
          | **Skipped (cached)** | ${{ steps.avatars.outputs.avatars_skipped }} |
          | **Failed** | ${{ steps.avatars.outputs.avatars_failed }} |
          | **Deleted (orphaned)** | ${{ steps.avatars.outputs.avatars_deleted }} |

          ### Source Files
          EOF

          # Parse and display file stats
          python3 << 'PYEOF'
          import json
          import os

          file_stats_json = """${{ steps.merge.outputs.file_stats }}"""
          
          if file_stats_json:
              try:
                  file_stats = json.loads(file_stats_json)
                  
                  with open(os.environ["GITHUB_STEP_SUMMARY"], "a") as f:
                      for filename, stats in file_stats.items():
                          status_emoji = {
                              "success": "‚úÖ",
                              "not_found": "‚ÑπÔ∏è",
                              "empty": "‚ö†Ô∏è",
                              "no_samples": "‚ö†Ô∏è",
                              "json_error": "‚ùå",
                              "error": "‚ùå"
                          }.get(stats.get("status"), "‚ùì")
                          
                          f.write(f"- {status_emoji} **{filename}** ({stats.get('type')}): {stats.get('count')} samples\n")
              except: 
                  pass
          PYEOF

          cat << 'EOF' >> $GITHUB_STEP_SUMMARY
          
          ## üñºÔ∏è Thumbnail Optimization
          
          | Metric | Count |
          |--------|-------|
          | **Processed** | ${{ steps.thumbs.outputs.thumbs_processed }} |
          | **Skipped (cached)** | ${{ steps.thumbs.outputs.thumbs_skipped }} |
          | **Failed** | ${{ steps.thumbs.outputs.thumbs_failed }} |
          | **Deleted (orphaned)** | ${{ steps.thumbs.outputs.thumbs_deleted }} |
          
          EOF
          
          if [ "${{ steps.thumbs.outputs.has_thumb_failures }}" == "true" ]; then
            echo "### ‚ö†Ô∏è Failed Thumbnails" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "The following thumbnails failed to process and are using original URLs.  See \`.metadata/thumbnail-failures.json\` for details:" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            if [ -f .metadata/thumbnail-failures.json ]; then
              python3 << 'PYEOF'
          import json
          import os
          
          with open(".metadata/thumbnail-failures.json") as f:
              failures = json.load(f)
          
          with open(os.environ["GITHUB_STEP_SUMMARY"], "a") as summary:
              summary.write("| Sample | Error |\n")
              summary.write("|--------|-------|\n")
              for failure in failures:
                  summary.write(f"| {failure['name']} | {failure['error']} |\n")
          PYEOF
            fi
          fi
          
          cat << 'EOF' >> $GITHUB_STEP_SUMMARY
          
          ## üìù Git Changes
          
          | Status | Result |
          |--------|--------|
          | **Changes Detected** | ${{ steps.check_changes.outputs.changes == 'true' && '‚úÖ Yes' || '‚ÑπÔ∏è No' }} |
          | **Committed & Pushed** | ${{ steps.check_changes.outputs.changes == 'true' && '‚úÖ Yes' || '‚ÑπÔ∏è No changes to commit' }} |
          
          ---
          
          ${{ steps.merge.outputs.has_errors == 'true' && '‚ö†Ô∏è **Warning**:  Merge completed with errors.  Check `.metadata/merge-errors.json` for details.' || '‚úÖ **Success**: All samples merged successfully!' }}
          EOF